ModelArguments(model_name_or_path='/data/slwang/FedPETuning-Lite/pretrain/nlp/roberta-base/', model_type='roberta', model_output_mode='seq_classification', config_name=None, tokenizer_name=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, ignore_mismatched_sizes=False, permutation_layers=False, client_model_layers=[0, 1, 2, 3, 4, 5], server_model_layers=[0, 1, 2, 3, 4, 5], tuning_type='adapter_roberta-base', lora_r=8, lora_alpha=8, prefix_token_num=16, bottleneck_dim=16)